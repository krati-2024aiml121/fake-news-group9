{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration for Fake News Detection\n",
    "\n",
    "This notebook provides comprehensive data exploration for the fake news detection dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install pandas scikit-learn matplotlib joblib openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Fake news dataset shape: (23481, 5)\n",
      "True news dataset shape: (21417, 5)\n",
      "Combined dataset shape: (44898, 5)\n",
      "Datasets loaded and combined successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "print(\"Loading datasets...\")\n",
    "\n",
    "# Load fake news data\n",
    "fake_df = pd.read_csv('Fake.csv')\n",
    "fake_df['label'] = 0  # 0 for fake news\n",
    "\n",
    "# Load true news data  \n",
    "true_df = pd.read_csv('True.csv')\n",
    "true_df['label'] = 1  # 1 for true news\n",
    "\n",
    "print(f\"Fake news dataset shape: {fake_df.shape}\")\n",
    "print(f\"True news dataset shape: {true_df.shape}\")\n",
    "\n",
    "# Combine both datasets\n",
    "df = pd.concat([fake_df, true_df], ignore_index=True)\n",
    "print(f\"Combined dataset shape: {df.shape}\")\n",
    "\n",
    "# Shuffle the dataset\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"Datasets loaded and combined successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Values Check\n",
    "\n",
    "This section performs a comprehensive check for missing values in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MISSING VALUES ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "üìä DATASET INFO:\n",
      "Total rows: 44,898\n",
      "Total columns: 5\n",
      "Columns: ['title', 'text', 'subject', 'date', 'label']\n",
      "\n",
      "üìã MISSING VALUES COUNT:\n",
      "‚úÖ title: No missing values\n",
      "‚úÖ text: No missing values\n",
      "‚úÖ subject: No missing values\n",
      "‚úÖ date: No missing values\n",
      "‚úÖ label: No missing values\n",
      "\n",
      "üìç ROW NUMBERS WITH MISSING VALUES:\n",
      "‚úÖ No missing values found in any column!\n",
      "\n",
      "üìà SUMMARY:\n",
      "Total missing values: 0\n",
      "Total cells in dataset: 224,490\n",
      "Percentage of missing data: 0.0000%\n",
      "\n",
      "‚úÖ No completely empty rows found\n"
     ]
    }
   ],
   "source": [
    "# Simple Missing Values Check\n",
    "\n",
    "def check_missing_values(df):\n",
    "   \n",
    "    print(\"=\"*80)\n",
    "    print(\"MISSING VALUES ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nüìä DATASET INFO:\")\n",
    "    print(f\"Total rows: {len(df):,}\")\n",
    "    print(f\"Total columns: {len(df.columns)}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Get missing value counts\n",
    "    missing_counts = df.isnull().sum()\n",
    "    \n",
    "    print(f\"\\nüìã MISSING VALUES COUNT:\")\n",
    "    for column in df.columns:\n",
    "        missing_count = missing_counts[column]\n",
    "        if missing_count > 0:\n",
    "            print(f\"üî¥ {column}: {missing_count:,} missing values ({missing_count/len(df)*100:.2f}%)\")\n",
    "        else:\n",
    "            print(f\"‚úÖ {column}: No missing values\")\n",
    "    \n",
    "    # Show specific row numbers for missing values\n",
    "    print(f\"\\nüìç ROW NUMBERS WITH MISSING VALUES:\")\n",
    "    has_missing = False\n",
    "    \n",
    "    for column in df.columns:\n",
    "        missing_rows = df[df[column].isnull()].index.tolist()\n",
    "        if missing_rows:\n",
    "            has_missing = True\n",
    "            print(f\"\\nüî¥ {column} (missing in {len(missing_rows)} rows):\")\n",
    "            if len(missing_rows) <= 20:\n",
    "                print(f\"   Row numbers: {missing_rows}\")\n",
    "            else:\n",
    "                print(f\"   First 20 rows: {missing_rows[:20]}\")\n",
    "                print(f\"   ... and {len(missing_rows) - 20} more rows\")\n",
    "    \n",
    "    if not has_missing:\n",
    "        print(\"‚úÖ No missing values found in any column!\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    total_missing = missing_counts.sum()\n",
    "    total_cells = len(df) * len(df.columns)\n",
    "    \n",
    "    print(f\"\\nüìà SUMMARY:\")\n",
    "    print(f\"Total missing values: {total_missing:,}\")\n",
    "    print(f\"Total cells in dataset: {total_cells:,}\")\n",
    "    print(f\"Percentage of missing data: {total_missing/total_cells*100:.4f}%\")\n",
    "    \n",
    "    # Check for completely empty rows\n",
    "    empty_rows = df.isnull().all(axis=1).sum()\n",
    "    if empty_rows > 0:\n",
    "        empty_row_indices = df[df.isnull().all(axis=1)].index.tolist()\n",
    "        print(f\"\\nüö® COMPLETELY EMPTY ROWS: {empty_rows}\")\n",
    "        print(f\"   Empty row numbers: {empty_row_indices}\")\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ No completely empty rows found\")\n",
    "\n",
    "# Check for missing values in the dataset\n",
    "check_missing_values(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing and Cleaning\n",
    "\n",
    "This section contains comprehensive text cleaning functions for preprocessing the news articles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading NLTK data...\n",
      "NLTK data downloaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import additional libraries for text processing\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Download required NLTK data\n",
    "print(\"Downloading NLTK data...\")\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "print(\"NLTK data downloaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Text Cleaning Functions\n",
    "\n",
    "def remove_urls(text):\n",
    "    \"\"\"Remove URLs from text\"\"\"\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub('', text)\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    \"\"\"Remove HTML tags from text\"\"\"\n",
    "    html_pattern = re.compile(r'<.*?>')\n",
    "    return html_pattern.sub('', text)\n",
    "\n",
    "def remove_emails(text):\n",
    "    \"\"\"Remove email addresses from text\"\"\"\n",
    "    email_pattern = re.compile(r'\\S+@\\S+')\n",
    "    return email_pattern.sub('', text)\n",
    "\n",
    "def remove_special_characters(text, keep_apostrophes=True):\n",
    "    \"\"\"Remove special characters while optionally keeping apostrophes for contractions\"\"\"\n",
    "    if keep_apostrophes:\n",
    "        # Keep apostrophes for contractions\n",
    "        pattern = r'[^a-zA-Z0-9\\s\\']'\n",
    "    else:\n",
    "        pattern = r'[^a-zA-Z0-9\\s]'\n",
    "    return re.sub(pattern, '', text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Text Cleaning Pipeline\n",
    "\n",
    "def clean_text_basic(text):\n",
    "    \"\"\"\n",
    "    Basic text cleaning pipeline\n",
    "    Suitable for models that need to preserve some original structure\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string and lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Remove URLs, emails, and HTML tags\n",
    "    text = remove_urls(text)\n",
    "    text = remove_emails(text)\n",
    "    text = remove_html_tags(text)\n",
    "\n",
    "    \n",
    "    # Remove special characters but keep apostrophes\n",
    "    text = remove_special_characters(text, keep_apostrophes=True)\n",
    "\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying text cleaning to the dataset...\n",
      "This may take a few minutes for large datasets...\n",
      "Text cleaning completed!\n",
      "Dataset shape after adding cleaned columns: (44898, 9)\n",
      "New columns: ['title_original', 'text_original', 'title_basic', 'text_basic']\n"
     ]
    }
   ],
   "source": [
    "# Apply text cleaning to the dataset\n",
    "\n",
    "print(\"Applying text cleaning to the dataset...\")\n",
    "print(\"This may take a few minutes for large datasets...\")\n",
    "\n",
    "# Create copies of original text columns for comparison\n",
    "df['title_original'] = df['title'].copy()\n",
    "df['text_original'] = df['text'].copy()\n",
    "\n",
    "# Apply different levels of cleaning\n",
    "# Basic cleaning (good for neural networks)\n",
    "df['title_basic'] = df['title'].apply(clean_text_basic)\n",
    "df['text_basic'] = df['text'].apply(clean_text_basic)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Text cleaning completed!\")\n",
    "print(f\"Dataset shape after adding cleaned columns: {df.shape}\")\n",
    "print(f\"New columns: {[col for col in df.columns if 'title_' in col or 'text_' in col]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first five rows                                                title  \\\n",
      "0  Ben Stein Calls Out 9th Circuit Court: Committ...   \n",
      "1  Trump drops Steve Bannon from National Securit...   \n",
      "2  Puerto Rico expects U.S. to lift Jones Act shi...   \n",
      "3   OOPS: Trump Just Accidentally Confirmed He Le...   \n",
      "4  Donald Trump heads for Scotland to reopen a go...   \n",
      "\n",
      "                                                text       subject  \\\n",
      "0  21st Century Wire says Ben Stein, reputable pr...       US_News   \n",
      "1  WASHINGTON (Reuters) - U.S. President Donald T...  politicsNews   \n",
      "2  (Reuters) - Puerto Rico Governor Ricardo Rosse...  politicsNews   \n",
      "3  On Monday, Donald Trump once again embarrassed...          News   \n",
      "4  GLASGOW, Scotland (Reuters) - Most U.S. presid...  politicsNews   \n",
      "\n",
      "                  date  label  \\\n",
      "0    February 13, 2017      0   \n",
      "1       April 5, 2017       1   \n",
      "2  September 27, 2017       1   \n",
      "3         May 22, 2017      0   \n",
      "4       June 24, 2016       1   \n",
      "\n",
      "                                      title_original  \\\n",
      "0  Ben Stein Calls Out 9th Circuit Court: Committ...   \n",
      "1  Trump drops Steve Bannon from National Securit...   \n",
      "2  Puerto Rico expects U.S. to lift Jones Act shi...   \n",
      "3   OOPS: Trump Just Accidentally Confirmed He Le...   \n",
      "4  Donald Trump heads for Scotland to reopen a go...   \n",
      "\n",
      "                                       text_original  \\\n",
      "0  21st Century Wire says Ben Stein, reputable pr...   \n",
      "1  WASHINGTON (Reuters) - U.S. President Donald T...   \n",
      "2  (Reuters) - Puerto Rico Governor Ricardo Rosse...   \n",
      "3  On Monday, Donald Trump once again embarrassed...   \n",
      "4  GLASGOW, Scotland (Reuters) - Most U.S. presid...   \n",
      "\n",
      "                                         title_basic  \\\n",
      "0  ben stein calls out 9th circuit court committe...   \n",
      "1  trump drops steve bannon from national securit...   \n",
      "2  puerto rico expects us to lift jones act shipp...   \n",
      "3   oops trump just accidentally confirmed he lea...   \n",
      "4  donald trump heads for scotland to reopen a go...   \n",
      "\n",
      "                                          text_basic  \n",
      "0  21st century wire says ben stein reputable pro...  \n",
      "1  washington reuters  us president donald trump ...  \n",
      "2  reuters  puerto rico governor ricardo rossello...  \n",
      "3  on monday donald trump once again embarrassed ...  \n",
      "4  glasgow scotland reuters  most us presidential...  \n"
     ]
    }
   ],
   "source": [
    "print(f\"first five rows {df.head(5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Text Preprocessing Functions\n",
    "\n",
    "# Initialize stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Get English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character used by WordNetLemmatizer\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def remove_stopwords(text, custom_stopwords=None):\n",
    "    \"\"\"Remove stopwords from text\"\"\"\n",
    "    words = word_tokenize(text.lower())\n",
    "    \n",
    "    # Combine default stopwords with custom ones if provided\n",
    "    stopwords_to_remove = stop_words\n",
    "    if custom_stopwords:\n",
    "        stopwords_to_remove = stop_words.union(set(custom_stopwords))\n",
    "    \n",
    "    filtered_words = [word for word in words if word not in stopwords_to_remove]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "def stem_text(text):\n",
    "    \"\"\"Apply stemming to text\"\"\"\n",
    "    words = word_tokenize(text.lower())\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \"\"\"Apply lemmatization to text with POS tagging\"\"\"\n",
    "    words = word_tokenize(text.lower())\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "def remove_short_words(text, min_length=2):\n",
    "    \"\"\"Remove words shorter than min_length\"\"\"\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if len(word) >= min_length]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "def clean_text_advanced(text, remove_stops=True, use_lemmatization=True):\n",
    "    \"\"\"\n",
    "    Advanced text cleaning pipeline\n",
    "    Suitable for traditional ML models with TF-IDF\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Start with basic cleaning\n",
    "    text = clean_text_basic(text)\n",
    "    \n",
    "    # Remove stopwords if requested\n",
    "    if remove_stops:\n",
    "        text = remove_stopwords(text)\n",
    "    \n",
    "    # Apply stemming or lemmatization\n",
    "    if use_lemmatization:\n",
    "        text = lemmatize_text(text)\n",
    "    else:\n",
    "        text = stem_text(text)\n",
    "    \n",
    "    # Remove short words\n",
    "    text = remove_short_words(text, min_length=2)\n",
    "\n",
    "    \n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
